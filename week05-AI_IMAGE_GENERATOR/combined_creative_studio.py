import streamlit as st
import torch
from diffusers import AutoPipelineForText2Image, LCMScheduler
from streamlit_webrtc import webrtc_streamer
import av
import cv2
import numpy as np
from PIL import Image, ImageDraw
import io
import base64
import os
import time
import hashlib
import random
import threading
import pickle

# å…¨å±€å˜é‡ç”¨äºè§†é¢‘å¤„ç†ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
current_overlay_image = None
overlay_lock = threading.Lock()

# ç”¨æ–‡ä»¶æ¥ä¼ é€’å›¾ç‰‡ï¼ˆç¡®ä¿è·¨çº¿ç¨‹å¯è®¿é—®ï¼‰
OVERLAY_CACHE_FILE = "temp_overlay.pkl"

def set_overlay_image(image):
    """çº¿ç¨‹å®‰å…¨çš„è®¾ç½®overlayå›¾ç‰‡ï¼ˆä½¿ç”¨æ–‡ä»¶ç¼“å­˜ï¼‰"""
    global current_overlay_image
    try:
        with overlay_lock:
            current_overlay_image = image
            # åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
            if image is not None:
                with open(OVERLAY_CACHE_FILE, 'wb') as f:
                    pickle.dump(image, f)
                print(f"DEBUG: Set overlay image - SUCCESS, saved to file")
            else:
                if os.path.exists(OVERLAY_CACHE_FILE):
                    os.remove(OVERLAY_CACHE_FILE)
                print(f"DEBUG: Set overlay image - NONE, removed file")
    except Exception as e:
        print(f"DEBUG: Set overlay error: {e}")

def get_overlay_image():
    """çº¿ç¨‹å®‰å…¨çš„è·å–overlayå›¾ç‰‡ï¼ˆä¼˜å…ˆä»æ–‡ä»¶è¯»å–ï¼‰"""
    global current_overlay_image
    try:
        # é¦–å…ˆå°è¯•ä»æ–‡ä»¶è¯»å–
        if os.path.exists(OVERLAY_CACHE_FILE):
            with open(OVERLAY_CACHE_FILE, 'rb') as f:
                image = pickle.load(f)
                print(f"DEBUG: Get overlay image from file - SUCCESS")
                return image
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å†…å­˜è¯»å–
        with overlay_lock:
            result = current_overlay_image
            print(f"DEBUG: Get overlay image from memory - {result is not None}")
            return result
            
    except Exception as e:
        print(f"DEBUG: Get overlay error: {e}")
        return None

# Page configuration
st.set_page_config(
    page_title="AI Creative Studio",
    page_icon="ğŸ¨",
    layout="wide"
)

# Initialize session state
if "pipeline" not in st.session_state:
    st.session_state["pipeline"] = None
    st.session_state["pipeline_loaded"] = False

if "generated_images" not in st.session_state:
    st.session_state["generated_images"] = []

if "selected_overlay_image" not in st.session_state:
    st.session_state["selected_overlay_image"] = None

# Main title
st.title("ğŸ¨ AI Creative Studio")
st.markdown("### Integrated Image Generation and Video Composition Creative Tool")

# Sidebar configuration
with st.sidebar:
    st.header("âš™ï¸ Feature Selection")
    
    # Feature mode selection
    feature_mode = st.selectbox(
        "Select Feature Mode",
        ["ğŸ¨ Image Generation", "ğŸ“¹ Video Filter", "ğŸ”„ Image+Video Composition"]
    )
    
    st.markdown("---")
    
    # GPU status
    st.subheader("ğŸ’» System Status")
    if torch.cuda.is_available():
        st.success(f"GPU: {torch.cuda.get_device_name(0)}")
        st.info(f"CUDA Version: {torch.version.cuda}")
    else:
        st.warning("Using CPU Mode")
    
    st.markdown("---")
    
    # Generated image gallery
    st.subheader("ğŸ–¼ï¸ Image Gallery")
    if st.session_state["generated_images"]:
        st.write(f"Generated {len(st.session_state['generated_images'])} images")
        
        # Display recent 3 image thumbnails
        for i, img in enumerate(st.session_state["generated_images"][-3:]):
            with st.container():
                st.image(img, caption=f"Image {len(st.session_state['generated_images'])-2+i}", width=150)
                if st.button(f"Use as Video Filter", key=f"use_img_{i}"):
                    st.session_state["selected_overlay_image"] = img
                    # Update global variable for video processing
                    set_overlay_image(img)
                    st.success("Selected as video filter image")
        
        if st.button("ğŸ—‘ï¸ Clear Image Gallery"):
            st.session_state["generated_images"] = []
            st.rerun()
    else:
        st.info("No generated images yet")

# åŠ è½½AIæ¨¡å‹çš„å‡½æ•°
@st.cache_resource
def load_image_pipeline():
    """Load image generation model"""
    try:
        model = 'lykon/dreamshaper-8-lcm'
        pipe = AutoPipelineForText2Image.from_pretrained(
            model, 
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        pipe.to("cuda" if torch.cuda.is_available() else "cpu")
        pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)
        return pipe
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

# Create cartoon animal overlay image
def create_cartoon_animal(animal_type, size=(100, 100)):
    """Create cartoon animal image based on animal type"""
    img = Image.new('RGBA', size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)
    
    center = (size[0]//2, size[1]//2)
    radius = min(size) // 3
    
    if animal_type == "cat":
        # çŒ«å’ª
        # å¤´éƒ¨ (æ©™è‰²)
        draw.ellipse([
            center[0] - radius, center[1] - radius,
            center[0] + radius, center[1] + radius
        ], fill=(255, 140, 0, 220))
        
        # çŒ«è€³æœµ (ä¸‰è§’å½¢)
        ear_size = radius // 2
        # å·¦è€³
        draw.polygon([
            center[0] - radius//2, center[1] - radius,
            center[0] - radius, center[1] - radius - ear_size,
            center[0], center[1] - radius - ear_size
        ], fill=(255, 140, 0, 220))
        # å³è€³
        draw.polygon([
            center[0] + radius//2, center[1] - radius,
            center[0] + radius, center[1] - radius - ear_size,
            center[0], center[1] - radius - ear_size
        ], fill=(255, 140, 0, 220))
        
        # çœ¼ç› (ç»¿è‰²)
        eye_size = radius // 4
        draw.ellipse([
            center[0] - radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] - radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 255, 0, 255))
        draw.ellipse([
            center[0] + radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] + radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 255, 0, 255))
        
        # é¼»å­ (å°ä¸‰è§’å½¢)
        nose_size = radius // 6
        draw.polygon([
            center[0], center[1] - nose_size//2,
            center[0] - nose_size//2, center[1] + nose_size//2,
            center[0] + nose_size//2, center[1] + nose_size//2
        ], fill=(255, 192, 203, 255))
        
    elif animal_type == "dog":
        # ç‹—ç‹—
        # å¤´éƒ¨ (æ£•è‰²)
        draw.ellipse([
            center[0] - radius, center[1] - radius,
            center[0] + radius, center[1] + radius
        ], fill=(139, 90, 43, 220))
        
        # ç‹—è€³æœµ (æ¤­åœ†å½¢ä¸‹å‚)
        ear_width = radius // 2
        ear_height = radius // 1.5
        # å·¦è€³
        draw.ellipse([
            center[0] - radius - ear_width//2, center[1] - radius//2 - ear_height//2,
            center[0] - radius + ear_width//2, center[1] - radius//2 + ear_height//2
        ], fill=(101, 67, 33, 220))
        # å³è€³
        draw.ellipse([
            center[0] + radius - ear_width//2, center[1] - radius//2 - ear_height//2,
            center[0] + radius + ear_width//2, center[1] - radius//2 + ear_height//2
        ], fill=(101, 67, 33, 220))
        
        # çœ¼ç› (é»‘è‰²)
        eye_size = radius // 4
        draw.ellipse([
            center[0] - radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] - radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        draw.ellipse([
            center[0] + radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] + radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        
        # é¼»å­ (é»‘è‰²åœ†)
        nose_size = radius // 6
        draw.ellipse([
            center[0] - nose_size//2, center[1] - nose_size//2,
            center[0] + nose_size//2, center[1] + nose_size//2
        ], fill=(0, 0, 0, 255))
        
        # å˜´å·´
        mouth_width = radius // 2
        draw.arc([
            center[0] - mouth_width//2, center[1] + nose_size,
            center[0] + mouth_width//2, center[1] + radius//2
        ], start=0, end=180, fill=(0, 0, 0, 255), width=2)
        
    elif animal_type == "rabbit":
        # å…”å­
        # å¤´éƒ¨ (ç™½è‰²)
        draw.ellipse([
            center[0] - radius, center[1] - radius,
            center[0] + radius, center[1] + radius
        ], fill=(255, 255, 255, 220))
        
        # å…”è€³æœµ (é•¿æ¤­åœ†å½¢)
        ear_width = radius // 3
        ear_height = radius
        # å·¦è€³
        draw.ellipse([
            center[0] - radius//2 - ear_width//2, center[1] - radius - ear_height,
            center[0] - radius//2 + ear_width//2, center[1] - radius
        ], fill=(255, 192, 203, 220))
        # å³è€³
        draw.ellipse([
            center[0] + radius//2 - ear_width//2, center[1] - radius - ear_height,
            center[0] + radius//2 + ear_width//2, center[1] - radius
        ], fill=(255, 192, 203, 220))
        
        # çœ¼ç› (çº¢è‰²)
        eye_size = radius // 4
        draw.ellipse([
            center[0] - radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] - radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(255, 0, 0, 255))
        draw.ellipse([
            center[0] + radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] + radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(255, 0, 0, 255))
        
        # é¼»å­ (ç²‰è‰²å°åœ†)
        nose_size = radius // 8
        draw.ellipse([
            center[0] - nose_size//2, center[1] - nose_size//2,
            center[0] + nose_size//2, center[1] + nose_size//2
        ], fill=(255, 192, 203, 255))
        
    elif animal_type == "sheep":
        # ç¾Šç¾Š
        # å¤´éƒ¨ (ç™½è‰²ï¼Œå¸¦å·æ¯›æ•ˆæœ)
        draw.ellipse([
            center[0] - radius, center[1] - radius,
            center[0] + radius, center[1] + radius
        ], fill=(255, 255, 255, 220))
        
        # å·æ¯›æ•ˆæœï¼ˆå°åœ†åœˆï¼‰
        wool_size = radius // 6
        for i in range(8):
            angle = i * 45  # æ¯45åº¦ä¸€ä¸ªå·æ¯›
            x = center[0] + int((radius - wool_size) * np.cos(np.radians(angle)))
            y = center[1] + int((radius - wool_size) * np.sin(np.radians(angle)))
            draw.ellipse([
                x - wool_size//2, y - wool_size//2,
                x + wool_size//2, y + wool_size//2
            ], fill=(240, 240, 240, 200))
        
        # çœ¼ç› (é»‘è‰²)
        eye_size = radius // 4
        draw.ellipse([
            center[0] - radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] - radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        draw.ellipse([
            center[0] + radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] + radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        
        # é¼»å­ (é»‘è‰²å°åœ†)
        nose_size = radius // 8
        draw.ellipse([
            center[0] - nose_size//2, center[1] - nose_size//2,
            center[0] + nose_size//2, center[1] + nose_size//2
        ], fill=(0, 0, 0, 255))
    
    else:
        # é»˜è®¤ç¬‘è„¸
        draw.ellipse([
            center[0] - radius, center[1] - radius,
            center[0] + radius, center[1] + radius
        ], fill=(255, 255, 0, 200))
        
        eye_size = radius // 4
        draw.ellipse([
            center[0] - radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] - radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        draw.ellipse([
            center[0] + radius//2 - eye_size//2, center[1] - radius//3 - eye_size//2,
            center[0] + radius//2 + eye_size//2, center[1] - radius//3 + eye_size//2
        ], fill=(0, 0, 0, 255))
        
        mouth_width = radius
        draw.arc([
            center[0] - mouth_width//2, center[1],
            center[0] + mouth_width//2, center[1] + radius//2
        ], start=0, end=180, fill=(0, 0, 0, 255), width=3)
    
    return img

# Generate animal type based on face position
def get_animal_for_face(face_x, face_y, face_w, face_h):
    """Generate fixed animal type based on face position features"""
    # ä½¿ç”¨äººè„¸ä½ç½®çš„å“ˆå¸Œå€¼æ¥ç¡®ä¿åŒä¸€ä¸ªä½ç½®æ€»æ˜¯ç”Ÿæˆç›¸åŒçš„åŠ¨ç‰©
    face_id = f"{face_x//20}_{face_y//20}_{face_w//10}_{face_h//10}"
    face_hash = hashlib.md5(face_id.encode()).hexdigest()
    
    # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸ºæ•°å­—å¹¶é€‰æ‹©åŠ¨ç‰©
    hash_num = int(face_hash[:8], 16)
    animals = ["cat", "dog", "rabbit", "sheep"]
    return animals[hash_num % len(animals)]

# åˆ›å»ºé»˜è®¤è¦†ç›–å›¾åƒï¼ˆä¿ç•™åŸæœ‰å‡½æ•°ä½œä¸ºå¤‡ç”¨ï¼‰
def create_default_overlay(size=(100, 100)):
    """åˆ›å»ºé»˜è®¤çš„è¦†ç›–å›¾åƒ"""
    return create_cartoon_animal("default", size)

# å›¾åƒè¦†ç›–å‡½æ•°
def overlay_image_on_frame(frame_img, overlay_img, x, y, scale=1.0):
    """åœ¨è§†é¢‘å¸§ä¸Šè¦†ç›–å›¾åƒ"""
    if overlay_img is None:
        return frame_img
    
    # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
    if isinstance(overlay_img, Image.Image):
        overlay_array = np.array(overlay_img)
    else:
        overlay_array = overlay_img
    
    # è°ƒæ•´è¦†ç›–å›¾åƒå¤§å°
    img_height, img_width = overlay_array.shape[:2]
    new_height = int(img_height * scale)
    new_width = int(img_width * scale)
    
    if new_height <= 0 or new_width <= 0:
        return frame_img
    
    # ç¼©æ”¾å›¾åƒ
    resized_img = cv2.resize(overlay_array, (new_width, new_height))
    
    # ç¡®ä¿åæ ‡åœ¨å›¾åƒè¾¹ç•Œå†…
    y1 = max(0, y - new_height // 2)
    y2 = min(frame_img.shape[0], y1 + new_height)
    x1 = max(0, x - new_width // 2)
    x2 = min(frame_img.shape[1], x1 + new_width)
    
    # è°ƒæ•´è¦†ç›–åŒºåŸŸ
    actual_height = y2 - y1
    actual_width = x2 - x1
    
    if actual_height <= 0 or actual_width <= 0:
        return frame_img
    
    img_roi = resized_img[:actual_height, :actual_width]
    
    # å¤„ç†é€æ˜åº¦
    if img_roi.shape[2] == 4:  # RGBA
        alpha = img_roi[:, :, 3] / 255.0
        for c in range(3):  # BGR
            frame_img[y1:y2, x1:x2, c] = (
                alpha * img_roi[:, :, c] + 
                (1 - alpha) * frame_img[y1:y2, x1:x2, c]
            )
    else:  # RGB
        frame_img[y1:y2, x1:x2] = img_roi[:, :, :3]
    
    return frame_img

# Video frame processing callback
def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    
    try:
        # äººè„¸æ£€æµ‹
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œå¤„ç†
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        
        # åœ¨æ¯ä¸ªäººè„¸ä¸Šæ·»åŠ å¯¹åº”çš„å¡é€šåŠ¨ç‰©è¦†ç›–å±‚
        for (x, y, w, h) in faces:
            # æ ¹æ®äººè„¸ä½ç½®è·å–å¯¹åº”çš„åŠ¨ç‰©ç±»å‹
            animal_type = get_animal_for_face(x, y, w, h)
            
            # åˆ›å»ºå¯¹åº”åŠ¨ç‰©çš„è¦†ç›–å›¾åƒ
            overlay_size = (max(w, 80), max(h, 80))
            overlay = create_cartoon_animal(animal_type, overlay_size)
            
            # è®¡ç®—è¦†ç›–ä½ç½®
            overlay_x = max(0, x + w//2 - overlay_size[0]//2)
            overlay_y = max(0, y + h//2 - overlay_size[1]//2)
            
            # ç¡®ä¿è¦†ç›–å±‚ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
            if overlay_x + overlay_size[0] > pil_img.width:
                overlay_x = pil_img.width - overlay_size[0]
            if overlay_y + overlay_size[1] > pil_img.height:
                overlay_y = pil_img.height - overlay_size[1]
            
            # ç²˜è´´è¦†ç›–å±‚
            pil_img.paste(overlay, (overlay_x, overlay_y), overlay)
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        img_array = np.array(pil_img)
        img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
    
    except Exception as e:
        # å¦‚æœäººè„¸æ£€æµ‹å¤±è´¥ï¼Œåœ¨å±å¹•ä¸­å¤®æ·»åŠ é»˜è®¤åŠ¨ç‰©å›¾åƒ
        center_x = img.shape[1] // 2
        center_y = img.shape[0] // 2
        
        # åˆ›å»ºé»˜è®¤åŠ¨ç‰©è¦†ç›–
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        overlay = create_cartoon_animal("cat", (100, 100))
        
        overlay_x = max(0, center_x - 50)
        overlay_y = max(0, center_y - 50)
        pil_img.paste(overlay, (overlay_x, overlay_y), overlay)
        
        img_array = np.array(pil_img)
        img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
    
    return av.VideoFrame.from_ndarray(img, format="bgr24")

# Image+Video composition mode dynamic video processing callback
def combined_video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    
    try:
        # è·å–å½“å‰æ—¶é—´ç”¨äºåŠ¨æ€æ•ˆæœ
        current_time = time.time()
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œå¤„ç†  
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        
        # è·å–æ»¤é•œå›¾ç‰‡ï¼ˆä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•ï¼‰
        overlay_img = get_overlay_image()
        
        # è°ƒè¯•ä¿¡æ¯ï¼šåœ¨ç”»é¢ä¸Šæ˜¾ç¤ºçŠ¶æ€
        cv2.putText(img, f"Overlay status: {'Found' if overlay_img else 'None'}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # åªæœ‰å½“ç”¨æˆ·çœŸæ­£ç”Ÿæˆäº†AIå›¾ç‰‡æ—¶æ‰è¿›è¡Œå¤„ç†
        if overlay_img is not None:
            cv2.putText(img, "AI Filter Active", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # äººè„¸æ£€æµ‹
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            cv2.putText(img, f"Faces detected: {len(faces)}", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            if len(faces) > 0:
                # åœ¨æ¯ä¸ªäººè„¸ä¸Šæ·»åŠ åŠ¨æ€æ•ˆæœçš„ç”Ÿæˆå›¾ç‰‡
                for (x, y, w, h) in faces:
                    # ç»˜åˆ¶äººè„¸æ£€æµ‹æ¡†ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
                    
                    # åˆ›å»ºåŠ¨æ€æ•ˆæœ
                    face_center_x = x + w // 2
                    face_center_y = y + h // 2
                    
                    # åŠ¨æ€ç¼©æ”¾æ•ˆæœï¼ˆå‘¼å¸æ•ˆæœï¼‰
                    scale_factor = 0.7 + 0.3 * abs(np.sin(current_time * 2))  # 0.7-1.0ä¹‹é—´å˜åŒ–
                    
                    # åŠ¨æ€ä½ç½®åç§»ï¼ˆè½»å¾®æ‘†åŠ¨ï¼‰
                    offset_x = int(8 * np.sin(current_time * 3))
                    offset_y = int(4 * np.cos(current_time * 4))
                    
                    # è®¡ç®—åŠ¨æ€å°ºå¯¸ï¼ˆè®©å›¾ç‰‡ç¨å¾®å¤§äºäººè„¸æ¡†ï¼‰
                    face_scale = 1.3  # å›¾ç‰‡æ¯”äººè„¸å¤§30%
                    base_size = max(w, h) * face_scale
                    dynamic_size = (int(base_size * scale_factor), int(base_size * scale_factor))
                    
                    try:
                        # è½¬æ¢ä¸ºPILè¿›è¡Œå›¾ç‰‡å¤„ç†
                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        pil_img = Image.fromarray(img_rgb)
                        
                        # è°ƒæ•´ç”Ÿæˆå›¾ç‰‡å¤§å°
                        resized_overlay = overlay_img.resize(dynamic_size, Image.Resampling.LANCZOS)
                        
                        # æ·»åŠ è½»å¾®æ—‹è½¬æ•ˆæœ
                        rotation_angle = 2 * np.sin(current_time * 1.5)  # -2åˆ°2åº¦æ‘†åŠ¨ï¼ˆå‡å°‘æ—‹è½¬å¹…åº¦ï¼‰
                        if abs(rotation_angle) > 0.3:  # é¿å…è¿‡å°çš„æ—‹è½¬
                            # åˆ›å»ºå¸¦alphaé€šé“çš„å›¾åƒç”¨äºæ—‹è½¬
                            if resized_overlay.mode != 'RGBA':
                                resized_overlay = resized_overlay.convert('RGBA')
                            resized_overlay = resized_overlay.rotate(rotation_angle, expand=True)
                        
                        # è®¡ç®—ç²˜è´´ä½ç½®ï¼ˆç¨å¾®å‘ä¸Šåç§»ï¼Œæ›´å¥½åœ°è¦†ç›–äººè„¸ï¼‰
                        paste_x = max(0, face_center_x - resized_overlay.width // 2 + offset_x)
                        paste_y = max(0, face_center_y - resized_overlay.height // 2 - int(h * 0.1) + offset_y)  # å‘ä¸Šåç§»10%äººè„¸é«˜åº¦
                        
                        # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
                        if paste_x + resized_overlay.width > pil_img.width:
                            paste_x = pil_img.width - resized_overlay.width
                        if paste_y + resized_overlay.height > pil_img.height:
                            paste_y = pil_img.height - resized_overlay.height
                        
                        # ç¡®ä¿æœ‰alphaé€šé“
                        if resized_overlay.mode != 'RGBA':
                            resized_overlay = resized_overlay.convert('RGBA')
                        
                        # ç²˜è´´åˆ°è§†é¢‘å¸§ä¸Š
                        pil_img.paste(resized_overlay, (paste_x, paste_y), resized_overlay)
                        
                        # è½¬æ¢å›OpenCVæ ¼å¼
                        img_array = np.array(pil_img)
                        img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
                        
                    except Exception as e:
                        cv2.putText(img, f"Paste Error: {str(e)[:30]}", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            
            else:
                # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œåœ¨å±å¹•ä¸­å¤®æ˜¾ç¤ºåŠ¨æ€æ•ˆæœ
                center_x = img.shape[1] // 2
                center_y = img.shape[0] // 2
                
                try:
                    # è½¬æ¢ä¸ºPILè¿›è¡Œå¤„ç†
                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    pil_img = Image.fromarray(img_rgb)
                    
                    # åŠ¨æ€æ•ˆæœ
                    scale_factor = 0.6 + 0.4 * abs(np.sin(current_time * 1.5))
                    rotation_angle = 8 * np.sin(current_time * 1)
                    
                    # è°ƒæ•´å›¾ç‰‡
                    dynamic_size = (int(120 * scale_factor), int(120 * scale_factor))
                    resized_overlay = overlay_img.resize(dynamic_size, Image.Resampling.LANCZOS)
                    
                    if abs(rotation_angle) > 0.5:
                        if resized_overlay.mode != 'RGBA':
                            resized_overlay = resized_overlay.convert('RGBA')
                        resized_overlay = resized_overlay.rotate(rotation_angle, expand=True)
                    
                    # ä½ç½®
                    paste_x = max(0, center_x - resized_overlay.width // 2)
                    paste_y = max(0, center_y - resized_overlay.height // 2)
                    
                    # ç²˜è´´
                    if resized_overlay.mode != 'RGBA':
                        resized_overlay = resized_overlay.convert('RGBA')
                    pil_img.paste(resized_overlay, (paste_x, paste_y), resized_overlay)
                    
                    # è½¬æ¢å›OpenCVæ ¼å¼
                    img_array = np.array(pil_img)
                    img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
                    
                except Exception as e:
                    cv2.putText(img, f"Center Error: {str(e)[:30]}", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
        
        else:
            # æ²¡æœ‰æ»¤é•œå›¾ç‰‡æ—¶ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
            cv2.putText(img, "Please generate AI filter first", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    except Exception as e:
        # é”™è¯¯å¤„ç†ï¼šåœ¨å›¾ç‰‡ä¸Šæ˜¾ç¤ºç®€å•çš„é”™è¯¯ä¿¡æ¯
        cv2.putText(img, f"Error: {str(e)[:30]}", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
    
    return av.VideoFrame.from_ndarray(img, format="bgr24")

# ä¸»è¦å†…å®¹åŒºåŸŸ
if feature_mode == "ğŸ¨ Image Generation":
    # Image generation mode
    st.header("ğŸ¨ AI Image Generation")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“ Input Description")
        
        # Preset prompts
        preset_prompts = {
            "Select preset...": "",
            "Cute Animals": "cute fluffy puppy playing in garden, soft lighting, adorable",
            "Beautiful Landscapes": "beautiful landscape with mountains, lakes, golden hour lighting",
            "Sci-fi Scenes": "futuristic cityscape with flying cars, neon lights, cyberpunk style",
            "Art Portraits": "portrait of wise wizard, detailed face, magical atmosphere, fantasy art",
            "Cartoon Style": "cartoon style illustration, colorful, friendly, animated character"
        }
        
        selected_preset = st.selectbox("Preset Prompts", list(preset_prompts.keys()))
        
        # Main prompt input
        if selected_preset != "Select preset...":
            default_prompt = preset_prompts[selected_preset]
        else:
            default_prompt = ""
        
        prompt = st.text_area(
            "Image Description (Prompt)",
            value=default_prompt,
            height=100,
            help="Describe in detail the image you want to generate"
        )
        
        # Generation parameters
        col_param1, col_param2 = st.columns(2)
        with col_param1:
            num_steps = st.slider("Inference Steps", 4, 20, 8)
        with col_param2:
            guidance_scale = st.slider("Guidance Scale", 1.0, 5.0, 2.0, 0.5)
        
        # Generate button
        generate_btn = st.button("ğŸ¨ Generate Image", type="primary", width="stretch")
    
    with col2:
        st.subheader("ğŸ–¼ï¸ Generation Results")
        
        if generate_btn and prompt.strip():
            # Load model
            if not st.session_state["pipeline_loaded"]:
                with st.spinner("Loading AI model..."):
                    pipeline = load_image_pipeline()
                    if pipeline:
                        st.session_state["pipeline"] = pipeline
                        st.session_state["pipeline_loaded"] = True
                        st.success("Model loaded successfully!")
                    else:
                        st.error("Model loading failed")
                        st.stop()
            
            # Generate image
            if st.session_state["pipeline"]:
                with st.spinner("Generating image..."):
                    try:
                        start_time = time.time()
                        
                        images = st.session_state["pipeline"](
                            prompt,
                            num_inference_steps=num_steps,
                            guidance_scale=guidance_scale,
                            num_images_per_prompt=1
                        ).images
                        
                        generation_time = time.time() - start_time
                        
                        # Save to session state
                        for image in images:
                            st.session_state["generated_images"].append(image)
                        
                        # Display generated images
                        for image in images:
                            st.image(image, caption=prompt, use_column_width=True)
                        
                        st.success(f"âœ… Generation completed! Time: {generation_time:.2f}s")
                        
                        # Add download button
                        img_buffer = io.BytesIO()
                        images[0].save(img_buffer, format='PNG')
                        img_data = img_buffer.getvalue()
                        
                        st.download_button(
                            label="ğŸ“¥ Download Image",
                            data=img_data,
                            file_name=f"ai_generated_{int(time.time())}.png",
                            mime="image/png",
                            width="stretch"
                        )
                        
                    except Exception as e:
                        st.error(f"Generation failed: {e}")
        
        elif generate_btn and not prompt.strip():
            st.warning("è¯·è¾“å…¥å›¾ç‰‡æè¿°")
        
        # æ˜¾ç¤ºæœ€è¿‘ç”Ÿæˆçš„å›¾ç‰‡
        if st.session_state["generated_images"]:
            st.markdown("### æœ€è¿‘ç”Ÿæˆçš„å›¾ç‰‡")
            latest_image = st.session_state["generated_images"][-1]
            st.image(latest_image, use_container_width=True)

elif feature_mode == "ğŸ“¹ Video Filter":
    # Video filter mode
    st.header("ğŸ“¹ Real-time Video Filter")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“· æ‘„åƒå¤´é¢„è§ˆ")
        
        # è§†é¢‘æµç»„ä»¶
        webrtc_streamer(
            key="video_filter",
            video_frame_callback=video_frame_callback,
            rtc_configuration={
                "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
            }
        )
    
    with col2:
        st.subheader("ğŸ­ æ»¤é•œè®¾ç½®")
        
        # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„è¦†ç›–å›¾åƒ
        if st.session_state["selected_overlay_image"]:
            st.write("å½“å‰æ»¤é•œå›¾ç‰‡:")
            st.image(st.session_state["selected_overlay_image"], width=150)
            
            if st.button("ğŸ—‘ï¸ ç§»é™¤æ»¤é•œå›¾ç‰‡"):
                st.session_state["selected_overlay_image"] = None
                st.rerun()
        else:
            st.info("ä½¿ç”¨é»˜è®¤ç¬‘è„¸æ»¤é•œ")
            default_img = create_default_overlay((100, 100))
            st.image(default_img, width=150)
        
        st.markdown("### ğŸ“– ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1. ç‚¹å‡» **START** å¯åŠ¨æ‘„åƒå¤´
        2. å…è®¸æµè§ˆå™¨è®¿é—®æ‘„åƒå¤´æƒé™
        3. é¢å‘æ‘„åƒå¤´ï¼Œåº”ç”¨ä¼šè‡ªåŠ¨æ£€æµ‹äººè„¸
        4. æ»¤é•œå›¾ç‰‡ä¼šè¦†ç›–åœ¨æ£€æµ‹åˆ°çš„äººè„¸ä½ç½®
        5. ä»å›¾ç‰‡åº“é€‰æ‹©è‡ªå®šä¹‰æ»¤é•œå›¾ç‰‡
        """)

else:  # Image+Video composition mode
    # Composition mode
    st.header("ğŸ”„ Image Generation + Video Filter Composition")
    
    # Upper part: Image generation
    st.subheader("ğŸ¨ Step 1: Generate Filter Images")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # Quick generation presets
        quick_prompts = {
            "ğŸ¶ Cartoon Dog": "cute cartoon dog head, friendly smile, colorful, simple design",
            "ğŸ± Cute Cat": "adorable cartoon cat face, big eyes, cute expression, pastel colors",
            "ğŸ¦„ Unicorn": "magical unicorn head, rainbow mane, sparkles, fantasy style",
            "ğŸ» Teddy Bear": "teddy bear face, brown fur, cute button nose, friendly expression",
            "ğŸ‘‘ Crown": "golden crown, jewels, royal, elegant design",
            "ğŸ­ Mask": "venetian carnival mask, decorative, colorful patterns",
            "ğŸŒ¸ Flower Decoration": "beautiful flower crown, cherry blossoms, soft colors, transparent background",
            "â­ Star Halo": "glowing star halo, magical sparkles, golden light, celestial design",
            "ğŸ€ Bow Tie": "cute bow tie, pastel ribbons, kawaii style, soft colors",
            "ğŸª Jester Hat": "colorful jester hat, bells, fun carnival style, bright colors"
        }
        
        st.write("Quick Generate Face Filter Images:")
        st.caption("ğŸ’¡ These images will be automatically applied to detected faces")
        
        for prompt_name, prompt_text in quick_prompts.items():
            if st.button(prompt_name, width="stretch"):
                # Load model and generate
                if not st.session_state["pipeline_loaded"]:
                    with st.spinner("Loading AI model..."):
                        pipeline = load_image_pipeline()
                        if pipeline:
                            st.session_state["pipeline"] = pipeline
                            st.session_state["pipeline_loaded"] = True
                
                if st.session_state["pipeline"]:
                    with st.spinner(f"Generating {prompt_name}..."):
                        try:
                            images = st.session_state["pipeline"](
                                prompt_text,
                                num_inference_steps=8,
                                guidance_scale=2.0,
                                num_images_per_prompt=1
                            ).images
                            
                            # Save and select as filter
                            for image in images:
                                st.session_state["generated_images"].append(image)
                                st.session_state["selected_overlay_image"] = image
                                # Update global variable for video processing
                                set_overlay_image(image)
                            
                            st.success(f"âœ… {prompt_name} generated and set as filter!")
                            st.rerun()
                            
                        except Exception as e:
                            st.error(f"Generation failed: {e}")
    
    with col2:
        # Custom prompts
        st.write("Or enter custom description:")
        st.caption("ğŸ’¡ Recommended keywords: transparent background, face decoration, head accessory")
        custom_prompt = st.text_input("Image Description", placeholder="e.g.: golden halo glowing light transparent background")
        
        if st.button("ğŸ¨ Generate Custom Filter") and custom_prompt.strip():
            # Same generation logic
            if not st.session_state["pipeline_loaded"]:
                with st.spinner("Loading AI model..."):
                    pipeline = load_image_pipeline()
                    if pipeline:
                        st.session_state["pipeline"] = pipeline
                        st.session_state["pipeline_loaded"] = True
            
            if st.session_state["pipeline"]:
                with st.spinner("Generating image..."):
                    try:
                        images = st.session_state["pipeline"](
                            custom_prompt,
                            num_inference_steps=8,
                            guidance_scale=2.0,
                            num_images_per_prompt=1
                        ).images
                        
                        for image in images:
                            st.session_state["generated_images"].append(image)
                            st.session_state["selected_overlay_image"] = image
                            # Update global variable for video processing
                            set_overlay_image(image)
                        
                        st.success("âœ… Custom filter generated successfully!")
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"Generation failed: {e}")
    
    st.markdown("---")
    
    # Lower part: Video filter
    st.subheader("ğŸ“¹ Step 2: Apply Video Filter")
    
    col3, col4 = st.columns([2, 1])
    
    with col3:
        # Video stream (using specialized dynamic effects callback)
        webrtc_streamer(
            key="combined_filter",
            video_frame_callback=combined_video_frame_callback,
            rtc_configuration={
                "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
            }
        )
    
    with col4:
        st.write("Current Filter:")
        if st.session_state["selected_overlay_image"]:
            st.image(st.session_state["selected_overlay_image"], width=150)
            st.success("âœ… ä½¿ç”¨AIç”Ÿæˆçš„æ»¤é•œå›¾ç‰‡")
            
            # åŠ¨æ€æ•ˆæœè¯´æ˜
            st.info("""
            ğŸŒŸ **AIå›¾ç‰‡äººè„¸åº”ç”¨å·²å¯ç”¨ï¼**
            - ğŸ¯ AIå›¾ç‰‡æ™ºèƒ½è´´åˆäººè„¸
            - ğŸ”„ åŠ¨æ€å‘¼å¸ç¼©æ”¾æ•ˆæœ
            - ğŸª è½»å¾®æ‘†åŠ¨æ—‹è½¬
            - ğŸ‘¤ è‡ªåŠ¨äººè„¸è¿½è¸ªè¦†ç›–
            - ï¿½ æ™ºèƒ½å°ºå¯¸è°ƒæ•´ï¼ˆæ¯”äººè„¸å¤§30%ï¼‰
            """)
        else:
            default_img = create_default_overlay((100, 100))
            st.image(default_img, width=150)
            st.info("Please generate filter image first")
            
        # Regenerate button
        if st.button("ğŸ”„ Reselect Filter", width="stretch"):
            st.session_state["selected_overlay_image"] = None
            # Reset global variable
            set_overlay_image(None)
            st.rerun()

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
<p>ğŸ¨ AI Creative Studio | Image Generation + Video Filter Composition Tool</p>
<p>Powered by Stable Diffusion + OpenCV + WebRTC Technology</p>
</div>
""", unsafe_allow_html=True)